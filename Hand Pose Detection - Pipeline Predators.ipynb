{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Rough implement",
      "provenance": [],
      "collapsed_sections": [
        "Nw_3Lte8svwX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBB9FItQ5MLh"
      },
      "source": [
        "# Dependencies and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVoXNgtcufRZ"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import fnmatch\n",
        "import pickle\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "!pip install simpleaudio\n",
        "import simpleaudio as sa\n",
        "!pip install mediapipe\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import load_img\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i40tUxiP2yR"
      },
      "source": [
        "pip freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRsstGr8nJi1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aufDyA9CQqj3"
      },
      "source": [
        "if os.path.isdir('/content/sample_data'):\n",
        "  shutil.rmtree('/content/sample_data', ignore_errors=True)\n",
        "\n",
        "!unzip /content/drive/MyDrive/multiview_hand_pose_dataset_uploaded_v2.zip       #takes ~7-8 mins\n",
        "clear_output()\n",
        "print('Dataset unzipped!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QFTamAGUZUY"
      },
      "source": [
        "shutil.rmtree('augmented_samples', ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27_BSpr40bq"
      },
      "source": [
        "# Projected coordinates to webcam frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y4Hn5tqoo5x"
      },
      "source": [
        "def recursive_glob(rootdir='.', pattern='*'):\n",
        "  matches = []\n",
        "  for root, dirnames, filenames in os.walk(rootdir):\n",
        "    for filename in fnmatch.filter(filenames, pattern):\n",
        "      matches.append(os.path.join(root, filename))\n",
        "  return matches\n",
        "\n",
        "def natural_sort(l): \n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "def readAnnotation3D(file):\n",
        "  f = open(file, \"r\")\n",
        "  an = []\n",
        "  for l in f:\n",
        "    l = l.split()\n",
        "    an.append((float(l[1]),float(l[2]), float(l[3])))\n",
        "  return np.array(an, dtype=float)\n",
        "\n",
        "def readAnnotation2D(file):\n",
        "  f = open(file, \"r\")\n",
        "  an = []\n",
        "  for l in f:\n",
        "    l = l.split()\n",
        "    an.append((float(l[1]),float(l[2])))\n",
        "  return np.array(an, dtype=float)\n",
        "\n",
        "def saveAnnotation(jointCamPath, positions):\n",
        "  fOut = open(jointCamPath, 'w')\n",
        "  fOut.write(\"F4_KNU1_A \" + str(positions[0][0]) + \" \" + str(positions[0][1]) + \"\\n\")\n",
        "  fOut.write(\"F4_KNU1_B \" + str(positions[1][0]) + \" \" + str(positions[1][1]) + \"\\n\")\n",
        "  fOut.write(\"F4_KNU2_A \" + str(positions[2][0]) + \" \" + str(positions[2][1]) + \"\\n\")\n",
        "  fOut.write(\"F4_KNU3_A \" + str(positions[3][0]) + \" \" + str(positions[3][1]) + \"\\n\")\n",
        "\n",
        "  fOut.write(\"F3_KNU1_A \" + str(positions[4][0]) + \" \" + str(positions[4][1]) + \"\\n\")\n",
        "  fOut.write(\"F3_KNU1_B \" + str(positions[5][0]) + \" \" + str(positions[5][1]) + \"\\n\")\n",
        "  fOut.write(\"F3_KNU2_A \" + str(positions[6][0]) + \" \" + str(positions[6][1]) + \"\\n\")\n",
        "  fOut.write(\"F3_KNU3_A \" + str(positions[7][0]) + \" \" + str(positions[7][1]) + \"\\n\")\n",
        "\n",
        "  fOut.write(\"F1_KNU1_A \" + str(positions[8][0]) + \" \" + str(positions[8][1]) + \"\\n\")\n",
        "  fOut.write(\"F1_KNU1_B \" + str(positions[9][0]) + \" \" + str(positions[9][1]) + \"\\n\")\n",
        "  fOut.write(\"F1_KNU2_A \" + str(positions[10][0]) + \" \" + str(positions[10][1]) + \"\\n\")\n",
        "  fOut.write(\"F1_KNU3_A \" + str(positions[11][0]) + \" \" + str(positions[11][1]) + \"\\n\")\n",
        "\n",
        "  fOut.write(\"F2_KNU1_A \" + str(positions[12][0]) + \" \" + str(positions[12][1]) + \"\\n\")\n",
        "  fOut.write(\"F2_KNU1_B \" + str(positions[13][0]) + \" \" + str(positions[13][1]) + \"\\n\")\n",
        "  fOut.write(\"F2_KNU2_A \" + str(positions[14][0]) + \" \" + str(positions[14][1]) + \"\\n\")\n",
        "  fOut.write(\"F2_KNU3_A \" + str(positions[15][0]) + \" \" + str(positions[15][1]) + \"\\n\")\n",
        "\n",
        "  fOut.write(\"TH_KNU1_A \" + str(positions[16][0]) + \" \" + str(positions[16][1]) + \"\\n\")\n",
        "  fOut.write(\"TH_KNU1_B \" + str(positions[17][0]) + \" \" + str(positions[17][1]) + \"\\n\")\n",
        "  fOut.write(\"TH_KNU2_A \" + str(positions[18][0]) + \" \" + str(positions[18][1]) + \"\\n\")\n",
        "  fOut.write(\"TH_KNU3_A \" + str(positions[19][0]) + \" \" + str(positions[19][1]) + \"\\n\")\n",
        "  fOut.write(\"PALM_POSITION \" + str(positions[20][0]) + \" \" + str(positions[20][1]) + \"\\n\")\n",
        "  fOut.close()\n",
        "\n",
        "def getCameraMatrix():\n",
        "  Fx = 614.878\n",
        "  Fy = 615.479\n",
        "  Cx = 313.219\n",
        "  Cy = 231.288\n",
        "  cameraMatrix = np.array([[Fx, 0 , Cx],\n",
        "                           [0 , Fy, Cy],\n",
        "                           [0 , 0 , 1]])\n",
        "  return cameraMatrix\n",
        "\n",
        "def getDistCoeffs():\n",
        "  return np.array([0.092701, -0.175877, -0.0035687, -0.00302299, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTTjzKKGwahY"
      },
      "source": [
        "def generate2Dcoordinates():\n",
        "  pathToDataset=\"/content/annotated_frames/\"\n",
        "\n",
        "  cameraMatrix = getCameraMatrix()\n",
        "  distCoeffs = getDistCoeffs()\n",
        "\n",
        "  if os.path.isdir('2d_points'):\n",
        "    shutil.rmtree('2d_points', ignore_errors=True)\n",
        "  os.mkdir('2d_points')\n",
        "\n",
        "  # iterate sequences\n",
        "  for i in range(1,22):\n",
        "    subdir_path = '/content/2d_points/data_'+str(i)+'/'\n",
        "    os.mkdir(subdir_path)\n",
        "    # read the color frames\n",
        "    path = pathToDataset+\"data_\"+str(i)+\"/\"\n",
        "    colorFrames = recursive_glob(path, \"*_webcam_[0-9]*\")\n",
        "    colorFrames = natural_sort(colorFrames)\n",
        "    print(\"There are \"+str(len(colorFrames))+\" color frames on the sequence data_\"+str(i))\n",
        "    # read the calibrations for each camera\n",
        "    print (\"Loading calibration for ../calibrations/data_\"+str(i))\n",
        "    c_0_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_1/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_0_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_1/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_1_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_2/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_1_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_2/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_2_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_3/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_2_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_3/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_3_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_4/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_3_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_4/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "\n",
        "    rand_idx = random.randint(0, len(colorFrames))\n",
        "\n",
        "    for j in range(len(colorFrames)):\n",
        "      print(colorFrames[j])\n",
        "      toks1 = colorFrames[j].split(\"/\")\n",
        "      toks2 = toks1[4].split(\"_\")\n",
        "      jointPath = \"/\"+toks1[1]+\"/\"+toks1[2]+\"/\"+toks1[3]+\"/\"+toks2[0]+\"_joints.txt\"\n",
        "      \n",
        "      print(jointPath)\n",
        "      points3d = readAnnotation3D(jointPath)[0:21] # the last point is the normal\n",
        "\n",
        "      # project 3d LM points to the image plane\n",
        "      webcam_id = int(toks2[2].split(\".\")[0])-1\n",
        "      print (\"Calibration for webcam id: \"+str(webcam_id))\n",
        "      if webcam_id == 0:\n",
        "        rvec = c_0_0\n",
        "        tvec = c_0_1\n",
        "      elif webcam_id == 1:\n",
        "        rvec = c_1_0\n",
        "        tvec = c_1_1\n",
        "      elif webcam_id == 2:\n",
        "        rvec = c_2_0\n",
        "        tvec = c_2_1\n",
        "      elif webcam_id == 3:\n",
        "        rvec = c_3_0\n",
        "        tvec = c_3_1\n",
        "\n",
        "      points2d, _ = cv2.projectPoints(points3d, rvec, tvec, cameraMatrix, distCoeffs)\n",
        "\n",
        "\n",
        "      # HERE YOU CAN SAVE points2d TO A FILE IF YOU WANT\n",
        "      pathToSave = \"/content/2d_points/data_\"+str(i)+\"/\"+toks2[0]+\"_webcam_\"+toks2[2].split(\".\")[0]+\".txt\"\n",
        "      print(\"Saving 2d projections\"+pathToSave)\n",
        "      saveAnnotation(pathToSave, np.array(points2d).reshape(21,2))\n",
        "\n",
        "      # show a random sample of the sequence\n",
        "      show = True\n",
        "      if show and j > rand_idx and j < rand_idx+4:\n",
        "        img = cv2.imread(colorFrames[j])\n",
        "        for k in range(points2d.shape[0]):\n",
        "          cv2.circle(img, (int(points2d[k][0][0]), int(points2d[k][0][1])), 3, (0,0,255))\n",
        "        cv2_imshow(img)\n",
        "        cv2.waitKey(100)\n",
        "  clear_output()\n",
        "  print('2D points generated!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fL43Ue2qzv_"
      },
      "source": [
        "def generate3Dcoordinates():\n",
        "\n",
        "  pathToDataset=\"/content/annotated_frames/\"\n",
        "  ##### Make a dir to save new 3D points\n",
        "  if os.path.isdir('3d_points'):\n",
        "    shutil.rmtree('3d_points', ignore_errors=True)\n",
        "  os.mkdir('3d_points')\n",
        "\n",
        "  # iterate sequences\n",
        "  for i in range(1,22):\n",
        "    # make subdirs to save 3D coordinates\n",
        "    subdir_path = '/content/3d_points/data_'+str(i)+'/'\n",
        "    os.mkdir(subdir_path)\n",
        "    # read the color frames\n",
        "    path = pathToDataset+\"data_\"+str(i)+\"/\"\n",
        "    colorFrames = recursive_glob(path, \"*_webcam_[0-9]*\")\n",
        "    colorFrames = natural_sort(colorFrames)\n",
        "    print(\"There are \"+str(len(colorFrames))+\" color frames on the sequence data_\"+str(i))\n",
        "    # read the calibrations for each camera\n",
        "    print (\"Loading calibration for ../calibrations/data_\"+str(i))\n",
        "    c_0_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_1/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_0_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_1/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_1_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_2/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_1_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_2/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_2_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_3/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_2_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_3/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_3_0 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_4/rvec.pkl\",\"rb\"),encoding='bytes')\n",
        "    c_3_1 = pickle.load(open(\"/content/calibrations/data_\"+str(i)+\"/webcam_4/tvec.pkl\",\"rb\"),encoding='bytes')\n",
        "\n",
        "    rand_idx = random.randint(0, len(colorFrames))\n",
        "\n",
        "\n",
        "    for j in range(len(colorFrames)):\n",
        "      print(colorFrames[j])\n",
        "      toks1 = colorFrames[j].split(\"/\")\n",
        "      toks2 = toks1[4].split(\"_\")\n",
        "      jointPath = \"/\"+toks1[1]+\"/\"+toks1[2]+\"/\"+toks1[3]+\"/\"+toks2[0]+\"_joints.txt\"\n",
        "      print(jointPath)\n",
        "      points3d = readAnnotation3D(jointPath)[0:21] # the last point is the normal\n",
        "\n",
        "      # project 3d LM points to the camera coordiante frame\n",
        "      webcam_id = int(toks2[2].split(\".\")[0])-1\n",
        "      print(\"Calibration for webcam id: \"+str(webcam_id))\n",
        "      if webcam_id == 0:\n",
        "        rvec = c_0_0\n",
        "        tvec = c_0_1\n",
        "      elif webcam_id == 1:\n",
        "        rvec = c_1_0\n",
        "        tvec = c_1_1\n",
        "      elif webcam_id == 2:\n",
        "        rvec = c_2_0\n",
        "        tvec = c_2_1\n",
        "      elif webcam_id == 3:\n",
        "        rvec = c_3_0\n",
        "        tvec = c_3_1\n",
        "\n",
        "      R,_ = cv2.Rodrigues(rvec)\n",
        "      T = np.zeros((4,4))\n",
        "      for l in range(R.shape[0]):\n",
        "        for k in range(R.shape[1]):\n",
        "          T[l][k] = R[l][k]\n",
        "\n",
        "      for l in range(tvec.shape[0]):\n",
        "        T[l][3] = tvec[l]\n",
        "      T[3][3] = 1\n",
        "\n",
        "      file_path = subdir_path + toks1[4].split(\".\")[0] + '.txt'\n",
        "      text_file = open(file_path, \"w\")\n",
        "      print(file_path)\n",
        "      names = ['F4_KNU1_A', 'F4_KNU1_B', 'F4_KNU2_A', 'F4_KNU3_A', 'F3_KNU1_A', \n",
        "              'F3_KNU1_B', 'F3_KNU2_A', 'F3_KNU3_A', 'F1_KNU1_A', 'F1_KNU1_B', \n",
        "              'F1_KNU2_A', 'F1_KNU3_A', 'F2_KNU1_A', 'F2_KNU1_B', 'F2_KNU2_A', \n",
        "              'F2_KNU3_A', 'TH_KNU1_A', 'TH_KNU1_B', 'TH_KNU2_A', 'TH_KNU3_A', \n",
        "              'PALM_POSITION']\n",
        "      points3d_cam = []\n",
        "      \n",
        "      for k in range(len(points3d)):\n",
        "        p = np.array(points3d[k])\n",
        "        p = np.append(p, 1)\n",
        "        p_ = [names[k]] + list(np.matmul(T, p.transpose()))\n",
        "        points3d_cam.append(p_)\n",
        "        write_string = str(p_[0])+' '+str(p_[1])+' '+str(p_[2])+' '+str(p_[3])+'\\n'\n",
        "        text_file.write(write_string)\n",
        "      text_file.close()\n",
        "  clear_output()\n",
        "  print('3D points generated!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssdPawvCxOxg"
      },
      "source": [
        "generate2Dcoordinates()                                           # runs for ~ 3-4 mins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOoazGDJrhkH"
      },
      "source": [
        "generate3Dcoordinates()                                           # runs for ~ 5-6 min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsEVHn1W1YT8"
      },
      "source": [
        "# Setup for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1vB9kG4NqKv"
      },
      "source": [
        "def dataframe_gen(img_size):\n",
        "\n",
        "  h,w,_ = img_size\n",
        "  listoflists=[]\n",
        "\n",
        "  for i in range(1,22):\n",
        "    path = '/content/annotated_frames/data_'+str(i)+'/'\n",
        "    colorFrames = recursive_glob(path, \"*_webcam_[0-9]*\")\n",
        "    colorFrames = natural_sort(colorFrames)   \n",
        "    \n",
        "    for j in range(len(colorFrames)):\n",
        "      toks1 = colorFrames[j].split(\"/\")\n",
        "      toks2 = toks1[4].split(\".\")\n",
        "      jointPath = '/content/2d_points/'+toks1[3]+'/'+toks2[0]+\".txt\"\n",
        "      temp1 = []\n",
        "      temp1.append(colorFrames[j])\n",
        "\n",
        "      temp2 = readAnnotation2D(jointPath).ravel().tolist()\n",
        "      for i in range(len(temp2)):\n",
        "        if (i%2 is 0):\n",
        "          temp2[i] = temp2[i] * (w/640)\n",
        "        else:\n",
        "          temp2[i] = temp2[i] * (h/480)\n",
        "\n",
        "      temp1 = temp1 + temp2\n",
        "      listoflists.append(temp1)\n",
        "  \n",
        "  names = ['F4_KNU1_A', 'F4_KNU1_B', 'F4_KNU2_A', 'F4_KNU3_A', 'F3_KNU1_A', \n",
        "          'F3_KNU1_B', 'F3_KNU2_A', 'F3_KNU3_A', 'F1_KNU1_A', 'F1_KNU1_B', \n",
        "          'F1_KNU2_A', 'F1_KNU3_A', 'F2_KNU1_A', 'F2_KNU1_B', 'F2_KNU2_A', \n",
        "          'F2_KNU3_A', 'TH_KNU1_A', 'TH_KNU1_B', 'TH_KNU2_A', 'TH_KNU3_A', \n",
        "          'PALM_POSITION']\n",
        "  vars = ['x', 'y']\n",
        "  col = []\n",
        "  for i in names:\n",
        "    for j in vars:\n",
        "        col.append(i+'_'+j)\n",
        "  y_col = col.copy()\n",
        "  col = ['image_paths'] + col\n",
        "\n",
        "  dataf = pd.DataFrame(listoflists, columns = col)\n",
        "  return y_col, dataf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb1tJvggnXdc"
      },
      "source": [
        "input_img_size = (256,256,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBbbzG-keb-E"
      },
      "source": [
        "y_cols, df_init = dataframe_gen(img_size=input_img_size)\n",
        "# raw images are (480,640,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39wwxy_hHaB5"
      },
      "source": [
        "df = df_init.sample(n=20000,random_state=57)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMcNLdlwJISR"
      },
      "source": [
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                            validation_split=0.2)\n",
        "image_resize_tuple = (input_img_size[0],input_img_size[1])\n",
        "train_gen = datagen.flow_from_dataframe(dataframe = df,\n",
        "                                        directory = None,\n",
        "                                        x_col='image_paths',\n",
        "                                        y_col=y_cols,\n",
        "                                        target_size = image_resize_tuple,\n",
        "                                        batch_size = 40,\n",
        "                                        class_mode = 'other',\n",
        "                                        subset = 'training')\n",
        "val_gen = datagen.flow_from_dataframe(dataframe = df,\n",
        "                                      directory = None,\n",
        "                                      x_col='image_paths',\n",
        "                                      y_col=y_cols,\n",
        "                                      target_size = image_resize_tuple,\n",
        "                                      batch_size = 10,\n",
        "                                      class_mode = 'other',\n",
        "                                      subset = 'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPfqPpMjI-qo"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "\n",
        "    layers.Conv2D(input_shape=input_img_size,filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
        "    layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "    layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "    layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "    layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "\n",
        "      # Classifier Head\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(units=4000, activation=\"relu\"),\n",
        "    layers.Dropout(.2),\n",
        "    layers.Dense(units=1000, activation=\"relu\"),\n",
        "    layers.Dropout(.2),\n",
        "    layers.Dense(units=500, activation=\"relu\"),\n",
        "    layers.Dense(units=100, activation=\"relu\"),\n",
        "    layers.Dense(units=42, activation=\"relu\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok8YqgCrrY5Y"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TQW62Zfvnbl"
      },
      "source": [
        "# Restore the weights\n",
        "model.load_weights('/content/drive/MyDrive/AI_model/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcPqNM1uRdKD"
      },
      "source": [
        "hist = {'loss': [],\n",
        "        'root_mean_squared_error': [],\n",
        "        'val_loss': [],\n",
        "        'val_root_mean_squared_error': []}\n",
        "\n",
        "def runner(num_epoch):\n",
        "  for i in range(num_epoch):\n",
        "    history = model.fit(train_gen, validation_data=val_gen, epochs = 1, verbose=1)\n",
        "    hist['loss'] += history.history['loss']\n",
        "    hist['root_mean_squared_error'] += history.history['root_mean_squared_error']\n",
        "    hist['val_loss'] += history.history['val_loss']\n",
        "    hist['val_root_mean_squared_error'] += history.history['val_root_mean_squared_error']\n",
        "    clear_output()\n",
        "    print(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JmGhmmSc9CY"
      },
      "source": [
        "runner(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVcvjCFpsuNk"
      },
      "source": [
        "# plot train and val loss \n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title('mean_squared_error')\n",
        "plt.plot(hist['loss'], color='blue', label='train')\n",
        "plt.plot(hist['val_loss'], color='red', label='val')\n",
        "#plt.xlim(0, 20)\n",
        "plt.ylim(0, 10000)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z84ZzKEByK0r"
      },
      "source": [
        "# plot train and val metrics \n",
        "plt.figure(figsize=(8,4))\n",
        "plt.title('mean_squared_error')\n",
        "plt.plot(hist['root_mean_squared_error'], color='blue', label='train')\n",
        "plt.plot(hist['val_root_mean_squared_error'], color='red', label='val')\n",
        "#plt.xlim(0, 20)\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEFLWHoFqahu"
      },
      "source": [
        "# Save the weights\n",
        "model.save_weights('/content/drive/MyDrive/AI_model/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bGez4B9S8qC"
      },
      "source": [
        "# summarize feature map shapes\n",
        "for i in range(len(model.layers)):\n",
        "\tlayer = model.layers[i]\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' not in layer.name:\n",
        "\t\tcontinue\n",
        "\t# summarize output shape\n",
        "\tprint(i, layer.name, layer.output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDf_KK02TYvQ"
      },
      "source": [
        "def plotfeaturemaps(layernum,imgpath):\n",
        "  _model_ = tf.keras.Model(inputs=model.inputs, outputs=model.layers[layernum].output)\n",
        "  img = load_img(imgpath, target_size=(256, 256))\n",
        "\n",
        "  # convert the image to an array\n",
        "  img = tf.keras.utils.img_to_array(img)/255\n",
        "  # expand dimensions so that it represents a single 'sample'\n",
        "  img = tf.expand_dims(img, axis=0)\n",
        "\n",
        "  feature_maps = _model_.predict(img)\n",
        "  # print(np.shape(feature_maps))\n",
        "\n",
        "  # plot all 64 maps in an 8x8 squares\n",
        "  square = int(np.shape(feature_maps)[3]**0.5)\n",
        "  ix = 1\n",
        "  fig = plt.figure(figsize = (2*square, 2*square))\n",
        "  for _ in range(square):\n",
        "    for _ in range(square):\n",
        "      # specify subplot and turn off axis\n",
        "      ax = plt.subplot(square, square, ix)\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "\n",
        "      fm = feature_maps[0, :, :, ix-1]\n",
        "      min = np.min(fm)\n",
        "      max = np.max(fm)\n",
        "      # plot filter channel in grayscale\n",
        "      if  min != max:\n",
        "        plt.imshow((fm-min)/(max-min))\n",
        "      else:\n",
        "        plt.imshow((fm-min))\n",
        "      ix += 1\n",
        "  # show the figure\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxXWU1wpHdkm"
      },
      "source": [
        "for i in range(0,21):\n",
        "    print('Layer '+str(i+1))\n",
        "    plotfeaturemaps(i,'/content/annotated_frames/data_15/0_webcam_3.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcDX9IDM6TBj"
      },
      "source": [
        "# Live testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfAKpqxsZ9cZ"
      },
      "source": [
        "drawing = mp.solutions.drawing_utils                                            \n",
        "styles = mp.solutions.drawing_styles\n",
        "hands = mp.solutions.hands(model.get_weights())\n",
        "seconds = 1\n",
        "fs = 44100\n",
        "cap = cv2.VideoCapture(0)\n",
        "with hands.Hands() as hands:\n",
        "    while cap.isOpened():\n",
        "        success, image = cap.read()\n",
        "        if not success:\n",
        "            print(\"Ignoring empty camera frame.\")\n",
        "            # If loading a video, use 'break' instead of 'continue'.\n",
        "            continue\n",
        "\n",
        "        # To improve performance, optionally mark the image as not writeable to\n",
        "        # pass by reference.\n",
        "        image.flags.writeable = False\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        results = hands.process(image)\n",
        "\n",
        "        # Draw the hand annotations on the image.\n",
        "        image.flags.writeable = True\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        if results.multi_hand_landmarks:\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                drawing.draw_landmarks(\n",
        "                image,\n",
        "                hand_landmarks,\n",
        "                hands.HAND_CONNECTIONS,\n",
        "                styles.get_default_hand_landmarks_style(),\n",
        "                styles.get_default_hand_connections_style())\n",
        "                frequency1 = (hand_landmarks.landmark[0].x**2) *2000\n",
        "                frequency2 = (hand_landmarks.landmark[0].x**2) *4000\n",
        "                frequency3 = (hand_landmarks.landmark[0].y**2) *2000\n",
        "                frequency4 = (hand_landmarks.landmark[0].y**2) *4000\n",
        "                t = np.linspace(0, seconds, seconds * fs, False)\n",
        "                note1 = np.sin(frequency1 * t * np.pi)\n",
        "                note2 = np.cos(frequency2 * t * np.pi)\n",
        "                note3 = np.sin(frequency3 * t * np.pi)\n",
        "                note4 = np.cos(frequency4 * t * np.pi)\n",
        "                note = note1 + note2 + note3 + note4 \n",
        "                audio = note * (2**15 - 1) / np.max(np.abs(note))\n",
        "                audio = audio.astype(np.int16)\n",
        "                play_obj = sa.play_buffer(audio, 1, 2, fs)\n",
        "                \n",
        "        # Flip the image horizontally for a selfie-view display.\n",
        "        cv2_imshow('Live Testing', cv2.flip(image, 1))\n",
        "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
        "            play_obj.wait_done()\n",
        "            break\n",
        "cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}